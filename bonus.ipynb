{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('heart.csv')\n",
    "# Binary encoding for 'Sex' and 'ExerciseAngina'\n",
    "data['Sex'] = data['Sex'].map({'M': 1, 'F': 0})\n",
    "data['ExerciseAngina'] = data['ExerciseAngina'].map({'Y': 1, 'N': 0})\n",
    "# Extract features and labels\n",
    "X = data.drop(columns=['HeartDisease'])\n",
    "y = data['HeartDisease']\n",
    "\n",
    "# One-Hot Encoding for non-binary categorical features\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "if categorical_cols:  # Only encode if there are categorical features\n",
    "    encoded_array = encoder.fit_transform(X[categorical_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "    X = pd.concat([X.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "#X_val (10%) → Used for hyperparameter tuning.\n",
    "# X_test (20%) → Used for final evaluation.\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=2/3, random_state=42, stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 3, metric: euclidean, Validation Accuracy: 0.8696\n",
      "k: 3, metric: manhattan, Validation Accuracy: 0.9239\n",
      "k: 5, metric: euclidean, Validation Accuracy: 0.8913\n",
      "k: 5, metric: manhattan, Validation Accuracy: 0.9022\n",
      "k: 11, metric: euclidean, Validation Accuracy: 0.8913\n",
      "k: 11, metric: manhattan, Validation Accuracy: 0.8913\n",
      "\n",
      "Best Hyperparameters:\n",
      "Best k: 3\n",
      "Best distance metric: manhattan\n",
      "Best Validation Accuracy: 0.9239\n",
      "Test Accuracy: 0.8533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Training KNN model\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ---------------------------\n",
    "#  Hyperparameter Tuning\n",
    "# --------------------------\n",
    "\n",
    "k_values = [3, 5, 11]\n",
    "distance_metrics = ['euclidean', 'manhattan']\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_k = None\n",
    "best_metric = None\n",
    "best_model = None\n",
    "\n",
    "for k in k_values:\n",
    "    for metric in distance_metrics:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_val_pred = knn.predict(X_val_scaled)\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        print(f\"k: {k}, metric: {metric}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Update best model if current model is better\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_k = k\n",
    "            best_metric = metric\n",
    "            best_model = knn\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"Best k: {best_k}\")\n",
    "print(f\"Best distance metric: {best_metric}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluation on Test Set\n",
    "# ---------------------------\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# train logistic regression\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# predict on validation set\n",
    "val_set = logistic_model.predict(X_val)\n",
    "# compute accuracy\n",
    "accuracy = accuracy_score(y_val, val_set)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design Neural Network\n",
    "\n",
    "# Scale Data and Convert to PyTorch Tensors\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_test_scaled = scaler.transform(X_test)          \n",
    "X_val_scaled = scaler.transform(X_val)            \n",
    "\n",
    "# Convert the scaled NumPy arrays to PyTorch tensors.\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  \n",
    "\n",
    "x_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "x_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Definition\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, learning_rate, num_epochs):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model.learning_rate)\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_threshold = 5\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(model.num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for example, lab in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(example).squeeze()\n",
    "            loss = criterion(output, lab.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        val_loss = validate_model(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{model.num_epochs}], Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        if no_improve_epochs >= early_stopping_threshold:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for example, lab in val_loader:\n",
    "            output = model(example).squeeze()\n",
    "            loss = criterion(output, lab.float())\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for example, _ in test_loader:\n",
    "            output = model(example).squeeze()\n",
    "            predicted = (output >= 0.5).int()  # Threshold at 0.5 to decide between class 0 and 1\n",
    "            predictions.extend(predicted.numpy())\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.6202, Val Loss: 0.6110\n",
      "Epoch [2/100], Loss: 0.5911, Val Loss: 0.5831\n",
      "Epoch [3/100], Loss: 0.5735, Val Loss: 0.5555\n",
      "Epoch [4/100], Loss: 0.5480, Val Loss: 0.5302\n",
      "Epoch [5/100], Loss: 0.5154, Val Loss: 0.5055\n",
      "Epoch [6/100], Loss: 0.4947, Val Loss: 0.4809\n",
      "Epoch [7/100], Loss: 0.4869, Val Loss: 0.4567\n",
      "Epoch [8/100], Loss: 0.4591, Val Loss: 0.4346\n",
      "Epoch [9/100], Loss: 0.4476, Val Loss: 0.4148\n",
      "Epoch [10/100], Loss: 0.4303, Val Loss: 0.3986\n",
      "Epoch [11/100], Loss: 0.3883, Val Loss: 0.3835\n",
      "Epoch [12/100], Loss: 0.4645, Val Loss: 0.3705\n",
      "Epoch [13/100], Loss: 0.3744, Val Loss: 0.3622\n",
      "Epoch [14/100], Loss: 0.3525, Val Loss: 0.3527\n",
      "Epoch [15/100], Loss: 0.3852, Val Loss: 0.3439\n",
      "Epoch [16/100], Loss: 0.3440, Val Loss: 0.3365\n",
      "Epoch [17/100], Loss: 0.3738, Val Loss: 0.3301\n",
      "Epoch [18/100], Loss: 0.3272, Val Loss: 0.3252\n",
      "Epoch [19/100], Loss: 0.4456, Val Loss: 0.3206\n",
      "Epoch [20/100], Loss: 0.4441, Val Loss: 0.3176\n",
      "Epoch [21/100], Loss: 0.4471, Val Loss: 0.3174\n",
      "Epoch [22/100], Loss: 0.3658, Val Loss: 0.3152\n",
      "Epoch [23/100], Loss: 0.3159, Val Loss: 0.3112\n",
      "Epoch [24/100], Loss: 0.3355, Val Loss: 0.3076\n",
      "Epoch [25/100], Loss: 0.3218, Val Loss: 0.3035\n",
      "Epoch [26/100], Loss: 0.3169, Val Loss: 0.3012\n",
      "Epoch [27/100], Loss: 0.3072, Val Loss: 0.2986\n",
      "Epoch [28/100], Loss: 0.3382, Val Loss: 0.2967\n",
      "Epoch [29/100], Loss: 0.3170, Val Loss: 0.2948\n",
      "Epoch [30/100], Loss: 0.3150, Val Loss: 0.2931\n",
      "Epoch [31/100], Loss: 0.3792, Val Loss: 0.2921\n",
      "Epoch [32/100], Loss: 0.3192, Val Loss: 0.2895\n",
      "Epoch [33/100], Loss: 0.2998, Val Loss: 0.2894\n",
      "Epoch [34/100], Loss: 0.3000, Val Loss: 0.2890\n",
      "Epoch [35/100], Loss: 0.2978, Val Loss: 0.2878\n",
      "Epoch [36/100], Loss: 0.3127, Val Loss: 0.2873\n",
      "Epoch [37/100], Loss: 0.3436, Val Loss: 0.2861\n",
      "Epoch [38/100], Loss: 0.2933, Val Loss: 0.2864\n",
      "Epoch [39/100], Loss: 0.2932, Val Loss: 0.2859\n",
      "Epoch [40/100], Loss: 0.3181, Val Loss: 0.2855\n",
      "Epoch [41/100], Loss: 0.2904, Val Loss: 0.2845\n",
      "Epoch [42/100], Loss: 0.3154, Val Loss: 0.2841\n",
      "Epoch [43/100], Loss: 0.2917, Val Loss: 0.2856\n",
      "Epoch [44/100], Loss: 0.2890, Val Loss: 0.2866\n",
      "Epoch [45/100], Loss: 0.3038, Val Loss: 0.2864\n",
      "Epoch [46/100], Loss: 0.3360, Val Loss: 0.2852\n",
      "Epoch [47/100], Loss: 0.2884, Val Loss: 0.2854\n",
      "Early stopping at epoch 47\n",
      "Epoch [1/100], Loss: 0.4971, Val Loss: 0.3712\n",
      "Epoch [2/100], Loss: 0.3259, Val Loss: 0.2943\n",
      "Epoch [3/100], Loss: 0.3104, Val Loss: 0.2757\n",
      "Epoch [4/100], Loss: 0.3007, Val Loss: 0.2801\n",
      "Epoch [5/100], Loss: 0.2895, Val Loss: 0.2761\n",
      "Epoch [6/100], Loss: 0.3374, Val Loss: 0.2761\n",
      "Epoch [7/100], Loss: 0.3048, Val Loss: 0.2889\n",
      "Epoch [8/100], Loss: 0.3819, Val Loss: 0.2761\n",
      "Early stopping at epoch 8\n",
      "Epoch [1/100], Loss: 0.4485, Val Loss: 0.3218\n",
      "Epoch [2/100], Loss: 0.3639, Val Loss: 0.2987\n",
      "Epoch [3/100], Loss: 0.3470, Val Loss: 0.3536\n",
      "Epoch [4/100], Loss: 0.3237, Val Loss: 0.3126\n",
      "Epoch [5/100], Loss: 0.3018, Val Loss: 0.3206\n",
      "Epoch [6/100], Loss: 0.3126, Val Loss: 0.2705\n",
      "Epoch [7/100], Loss: 0.3048, Val Loss: 0.3250\n",
      "Epoch [8/100], Loss: 0.3142, Val Loss: 0.2920\n",
      "Epoch [9/100], Loss: 0.3295, Val Loss: 0.3586\n",
      "Epoch [10/100], Loss: 0.3357, Val Loss: 0.3028\n",
      "Epoch [11/100], Loss: 0.2975, Val Loss: 0.3815\n",
      "Early stopping at epoch 11\n",
      "Epoch [1/100], Loss: 0.6779, Val Loss: 0.6558\n",
      "Epoch [2/100], Loss: 0.6438, Val Loss: 0.6241\n",
      "Epoch [3/100], Loss: 0.6121, Val Loss: 0.5941\n",
      "Epoch [4/100], Loss: 0.5855, Val Loss: 0.5633\n",
      "Epoch [5/100], Loss: 0.5327, Val Loss: 0.5354\n",
      "Epoch [6/100], Loss: 0.5386, Val Loss: 0.5068\n",
      "Epoch [7/100], Loss: 0.4678, Val Loss: 0.4812\n",
      "Epoch [8/100], Loss: 0.4646, Val Loss: 0.4552\n",
      "Epoch [9/100], Loss: 0.4181, Val Loss: 0.4304\n",
      "Epoch [10/100], Loss: 0.4094, Val Loss: 0.4087\n",
      "Epoch [11/100], Loss: 0.4555, Val Loss: 0.3900\n",
      "Epoch [12/100], Loss: 0.3941, Val Loss: 0.3791\n",
      "Epoch [13/100], Loss: 0.5005, Val Loss: 0.3683\n",
      "Epoch [14/100], Loss: 0.3472, Val Loss: 0.3628\n",
      "Epoch [15/100], Loss: 0.3419, Val Loss: 0.3542\n",
      "Epoch [16/100], Loss: 0.3667, Val Loss: 0.3458\n",
      "Epoch [17/100], Loss: 0.3561, Val Loss: 0.3387\n",
      "Epoch [18/100], Loss: 0.3532, Val Loss: 0.3312\n",
      "Epoch [19/100], Loss: 0.3463, Val Loss: 0.3262\n",
      "Epoch [20/100], Loss: 0.3439, Val Loss: 0.3208\n",
      "Epoch [21/100], Loss: 0.3523, Val Loss: 0.3154\n",
      "Epoch [22/100], Loss: 0.3127, Val Loss: 0.3115\n",
      "Epoch [23/100], Loss: 0.3776, Val Loss: 0.3090\n",
      "Epoch [24/100], Loss: 0.3031, Val Loss: 0.3093\n",
      "Epoch [25/100], Loss: 0.3062, Val Loss: 0.3079\n",
      "Epoch [26/100], Loss: 0.3128, Val Loss: 0.3059\n",
      "Epoch [27/100], Loss: 0.2995, Val Loss: 0.3038\n",
      "Epoch [28/100], Loss: 0.3009, Val Loss: 0.3020\n",
      "Epoch [29/100], Loss: 0.3061, Val Loss: 0.2998\n",
      "Epoch [30/100], Loss: 0.2914, Val Loss: 0.2986\n",
      "Epoch [31/100], Loss: 0.3973, Val Loss: 0.2973\n",
      "Epoch [32/100], Loss: 0.2923, Val Loss: 0.2967\n",
      "Epoch [33/100], Loss: 0.2974, Val Loss: 0.2953\n",
      "Epoch [34/100], Loss: 0.2884, Val Loss: 0.2949\n",
      "Epoch [35/100], Loss: 0.3191, Val Loss: 0.2941\n",
      "Epoch [36/100], Loss: 0.3439, Val Loss: 0.2927\n",
      "Epoch [37/100], Loss: 0.2900, Val Loss: 0.2929\n",
      "Epoch [38/100], Loss: 0.3836, Val Loss: 0.2922\n",
      "Epoch [39/100], Loss: 0.2815, Val Loss: 0.2911\n",
      "Epoch [40/100], Loss: 0.2893, Val Loss: 0.2902\n",
      "Epoch [41/100], Loss: 0.2819, Val Loss: 0.2890\n",
      "Epoch [42/100], Loss: 0.3076, Val Loss: 0.2885\n",
      "Epoch [43/100], Loss: 0.2866, Val Loss: 0.2876\n",
      "Epoch [44/100], Loss: 0.2837, Val Loss: 0.2869\n",
      "Epoch [45/100], Loss: 0.3213, Val Loss: 0.2873\n",
      "Epoch [46/100], Loss: 0.2834, Val Loss: 0.2894\n",
      "Epoch [47/100], Loss: 0.2912, Val Loss: 0.2890\n",
      "Epoch [48/100], Loss: 0.2787, Val Loss: 0.2881\n",
      "Epoch [49/100], Loss: 0.2766, Val Loss: 0.2878\n",
      "Early stopping at epoch 49\n",
      "Epoch [1/100], Loss: 0.5156, Val Loss: 0.3720\n",
      "Epoch [2/100], Loss: 0.3298, Val Loss: 0.3028\n",
      "Epoch [3/100], Loss: 0.3834, Val Loss: 0.2994\n",
      "Epoch [4/100], Loss: 0.3146, Val Loss: 0.3130\n",
      "Epoch [5/100], Loss: 0.2975, Val Loss: 0.3192\n",
      "Epoch [6/100], Loss: 0.3213, Val Loss: 0.3026\n",
      "Epoch [7/100], Loss: 0.3068, Val Loss: 0.2876\n",
      "Epoch [8/100], Loss: 0.2861, Val Loss: 0.2960\n",
      "Epoch [9/100], Loss: 0.2721, Val Loss: 0.2962\n",
      "Epoch [10/100], Loss: 0.2642, Val Loss: 0.2717\n",
      "Epoch [11/100], Loss: 0.2755, Val Loss: 0.2635\n",
      "Epoch [12/100], Loss: 0.2811, Val Loss: 0.2659\n",
      "Epoch [13/100], Loss: 0.2583, Val Loss: 0.2802\n",
      "Epoch [14/100], Loss: 0.2600, Val Loss: 0.2555\n",
      "Epoch [15/100], Loss: 0.2419, Val Loss: 0.2423\n",
      "Epoch [16/100], Loss: 0.2406, Val Loss: 0.2424\n",
      "Epoch [17/100], Loss: 0.2669, Val Loss: 0.2479\n",
      "Epoch [18/100], Loss: 0.3110, Val Loss: 0.2461\n",
      "Epoch [19/100], Loss: 0.2532, Val Loss: 0.2625\n",
      "Epoch [20/100], Loss: 0.2482, Val Loss: 0.2458\n",
      "Early stopping at epoch 20\n",
      "Epoch [1/100], Loss: 0.4727, Val Loss: 0.4808\n",
      "Epoch [2/100], Loss: 0.5343, Val Loss: 0.3138\n",
      "Epoch [3/100], Loss: 0.5957, Val Loss: 0.4300\n",
      "Epoch [4/100], Loss: 0.4374, Val Loss: 0.3920\n",
      "Epoch [5/100], Loss: 0.3672, Val Loss: 0.3711\n",
      "Epoch [6/100], Loss: 0.2878, Val Loss: 0.3043\n",
      "Epoch [7/100], Loss: 0.2635, Val Loss: 0.2862\n",
      "Epoch [8/100], Loss: 0.2510, Val Loss: 0.3637\n",
      "Epoch [9/100], Loss: 0.2835, Val Loss: 0.3056\n",
      "Epoch [10/100], Loss: 0.3673, Val Loss: 0.3412\n",
      "Epoch [11/100], Loss: 0.2899, Val Loss: 0.4644\n",
      "Epoch [12/100], Loss: 0.2794, Val Loss: 0.3702\n",
      "Early stopping at epoch 12\n",
      "Epoch [1/100], Loss: 0.6610, Val Loss: 0.6233\n",
      "Epoch [2/100], Loss: 0.5930, Val Loss: 0.5679\n",
      "Epoch [3/100], Loss: 0.5673, Val Loss: 0.5204\n",
      "Epoch [4/100], Loss: 0.4872, Val Loss: 0.4812\n",
      "Epoch [5/100], Loss: 0.4852, Val Loss: 0.4441\n",
      "Epoch [6/100], Loss: 0.4332, Val Loss: 0.4144\n",
      "Epoch [7/100], Loss: 0.4395, Val Loss: 0.3892\n",
      "Epoch [8/100], Loss: 0.3681, Val Loss: 0.3703\n",
      "Epoch [9/100], Loss: 0.3626, Val Loss: 0.3533\n",
      "Epoch [10/100], Loss: 0.3448, Val Loss: 0.3398\n",
      "Epoch [11/100], Loss: 0.3304, Val Loss: 0.3292\n",
      "Epoch [12/100], Loss: 0.3178, Val Loss: 0.3214\n",
      "Epoch [13/100], Loss: 0.3181, Val Loss: 0.3150\n",
      "Epoch [14/100], Loss: 0.3115, Val Loss: 0.3111\n",
      "Epoch [15/100], Loss: 0.3634, Val Loss: 0.3079\n",
      "Epoch [16/100], Loss: 0.3075, Val Loss: 0.3081\n",
      "Epoch [17/100], Loss: 0.3977, Val Loss: 0.3065\n",
      "Epoch [18/100], Loss: 0.3071, Val Loss: 0.3070\n",
      "Epoch [19/100], Loss: 0.3019, Val Loss: 0.3049\n",
      "Epoch [20/100], Loss: 0.2962, Val Loss: 0.3021\n",
      "Epoch [21/100], Loss: 0.2928, Val Loss: 0.3005\n",
      "Epoch [22/100], Loss: 0.2927, Val Loss: 0.2987\n",
      "Epoch [23/100], Loss: 0.2993, Val Loss: 0.2974\n",
      "Epoch [24/100], Loss: 0.3015, Val Loss: 0.2959\n",
      "Epoch [25/100], Loss: 0.2983, Val Loss: 0.2940\n",
      "Epoch [26/100], Loss: 0.3296, Val Loss: 0.2910\n",
      "Epoch [27/100], Loss: 0.2860, Val Loss: 0.2922\n",
      "Epoch [28/100], Loss: 0.3022, Val Loss: 0.2916\n",
      "Epoch [29/100], Loss: 0.2883, Val Loss: 0.2921\n",
      "Epoch [30/100], Loss: 0.3482, Val Loss: 0.2911\n",
      "Epoch [31/100], Loss: 0.2789, Val Loss: 0.2910\n",
      "Early stopping at epoch 31\n",
      "Epoch [1/100], Loss: 0.4685, Val Loss: 0.3009\n",
      "Epoch [2/100], Loss: 0.3159, Val Loss: 0.2733\n",
      "Epoch [3/100], Loss: 0.2992, Val Loss: 0.2855\n",
      "Epoch [4/100], Loss: 0.2826, Val Loss: 0.2959\n",
      "Epoch [5/100], Loss: 0.2942, Val Loss: 0.2779\n",
      "Epoch [6/100], Loss: 0.2876, Val Loss: 0.2691\n",
      "Epoch [7/100], Loss: 0.3041, Val Loss: 0.2643\n",
      "Epoch [8/100], Loss: 0.2625, Val Loss: 0.2868\n",
      "Epoch [9/100], Loss: 0.2618, Val Loss: 0.2800\n",
      "Epoch [10/100], Loss: 0.2587, Val Loss: 0.2703\n",
      "Epoch [11/100], Loss: 0.2459, Val Loss: 0.2652\n",
      "Epoch [12/100], Loss: 0.2773, Val Loss: 0.2638\n",
      "Epoch [13/100], Loss: 0.2370, Val Loss: 0.2840\n",
      "Epoch [14/100], Loss: 0.2493, Val Loss: 0.2609\n",
      "Epoch [15/100], Loss: 0.2307, Val Loss: 0.2795\n",
      "Epoch [16/100], Loss: 0.3327, Val Loss: 0.2923\n",
      "Epoch [17/100], Loss: 0.2346, Val Loss: 0.3370\n",
      "Epoch [18/100], Loss: 0.2507, Val Loss: 0.2936\n",
      "Epoch [19/100], Loss: 0.2199, Val Loss: 0.2916\n",
      "Early stopping at epoch 19\n",
      "Epoch [1/100], Loss: 0.4620, Val Loss: 0.4545\n",
      "Epoch [2/100], Loss: 0.4193, Val Loss: 0.4978\n",
      "Epoch [3/100], Loss: 0.3522, Val Loss: 0.3860\n",
      "Epoch [4/100], Loss: 0.2985, Val Loss: 0.2524\n",
      "Epoch [5/100], Loss: 0.2997, Val Loss: 0.2774\n",
      "Epoch [6/100], Loss: 0.2653, Val Loss: 0.2649\n",
      "Epoch [7/100], Loss: 0.4060, Val Loss: 0.3121\n",
      "Epoch [8/100], Loss: 0.4691, Val Loss: 0.6686\n",
      "Epoch [9/100], Loss: 0.3973, Val Loss: 0.4700\n",
      "Early stopping at epoch 9\n",
      "Epoch [1/100], Loss: 0.6646, Val Loss: 0.6083\n",
      "Epoch [2/100], Loss: 0.5693, Val Loss: 0.5275\n",
      "Epoch [3/100], Loss: 0.4800, Val Loss: 0.4639\n",
      "Epoch [4/100], Loss: 0.4345, Val Loss: 0.4131\n",
      "Epoch [5/100], Loss: 0.3956, Val Loss: 0.3742\n",
      "Epoch [6/100], Loss: 0.3992, Val Loss: 0.3463\n",
      "Epoch [7/100], Loss: 0.3322, Val Loss: 0.3283\n",
      "Epoch [8/100], Loss: 0.3570, Val Loss: 0.3144\n",
      "Epoch [9/100], Loss: 0.3183, Val Loss: 0.3085\n",
      "Epoch [10/100], Loss: 0.3665, Val Loss: 0.3012\n",
      "Epoch [11/100], Loss: 0.2993, Val Loss: 0.2969\n",
      "Epoch [12/100], Loss: 0.3049, Val Loss: 0.2937\n",
      "Epoch [13/100], Loss: 0.3774, Val Loss: 0.2883\n",
      "Epoch [14/100], Loss: 0.3326, Val Loss: 0.2862\n",
      "Epoch [15/100], Loss: 0.3283, Val Loss: 0.2891\n",
      "Epoch [16/100], Loss: 0.2887, Val Loss: 0.2870\n",
      "Epoch [17/100], Loss: 0.2914, Val Loss: 0.2849\n",
      "Epoch [18/100], Loss: 0.2851, Val Loss: 0.2813\n",
      "Epoch [19/100], Loss: 0.3588, Val Loss: 0.2792\n",
      "Epoch [20/100], Loss: 0.3016, Val Loss: 0.2796\n",
      "Epoch [21/100], Loss: 0.3080, Val Loss: 0.2807\n",
      "Epoch [22/100], Loss: 0.2844, Val Loss: 0.2805\n",
      "Epoch [23/100], Loss: 0.2806, Val Loss: 0.2795\n",
      "Epoch [24/100], Loss: 0.2932, Val Loss: 0.2774\n",
      "Epoch [25/100], Loss: 0.2723, Val Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.3838, Val Loss: 0.2801\n",
      "Epoch [27/100], Loss: 0.2836, Val Loss: 0.2796\n",
      "Epoch [28/100], Loss: 0.2688, Val Loss: 0.2793\n",
      "Epoch [29/100], Loss: 0.2904, Val Loss: 0.2774\n",
      "Early stopping at epoch 29\n",
      "Epoch [1/100], Loss: 0.4144, Val Loss: 0.3086\n",
      "Epoch [2/100], Loss: 0.3148, Val Loss: 0.3005\n",
      "Epoch [3/100], Loss: 0.2804, Val Loss: 0.2937\n",
      "Epoch [4/100], Loss: 0.2709, Val Loss: 0.2873\n",
      "Epoch [5/100], Loss: 0.4844, Val Loss: 0.2691\n",
      "Epoch [6/100], Loss: 0.2897, Val Loss: 0.2637\n",
      "Epoch [7/100], Loss: 0.2712, Val Loss: 0.2928\n",
      "Epoch [8/100], Loss: 0.3113, Val Loss: 0.3009\n",
      "Epoch [9/100], Loss: 0.3042, Val Loss: 0.3093\n",
      "Epoch [10/100], Loss: 0.4053, Val Loss: 0.2970\n",
      "Epoch [11/100], Loss: 0.2671, Val Loss: 0.3024\n",
      "Early stopping at epoch 11\n",
      "Epoch [1/100], Loss: 0.6939, Val Loss: 0.6232\n",
      "Epoch [2/100], Loss: 0.4983, Val Loss: 0.7545\n",
      "Epoch [3/100], Loss: 0.4061, Val Loss: 0.3735\n",
      "Epoch [4/100], Loss: 0.5190, Val Loss: 0.5766\n",
      "Epoch [5/100], Loss: 0.4988, Val Loss: 0.4207\n",
      "Epoch [6/100], Loss: 0.4252, Val Loss: 0.5343\n",
      "Epoch [7/100], Loss: 0.4957, Val Loss: 0.3596\n",
      "Epoch [8/100], Loss: 0.3405, Val Loss: 0.4271\n",
      "Epoch [9/100], Loss: 0.4833, Val Loss: 0.4838\n",
      "Epoch [10/100], Loss: 0.4813, Val Loss: 2.9050\n",
      "Epoch [11/100], Loss: 0.7883, Val Loss: 1.4914\n",
      "Epoch [12/100], Loss: 0.8369, Val Loss: 0.6024\n",
      "Early stopping at epoch 12\n",
      "Best Hidden Size: 16\n",
      "Best Learning Rate: 0.001\n",
      "Best Validation Accuracy: 0.9130\n"
     ]
    }
   ],
   "source": [
    "# Tune Hyperparameters \n",
    "input_size = X_train.shape[1]\n",
    "num_epochs = 100\n",
    "hidden_sizes = [16, 32, 64, 128]\n",
    "lr_values = [0.001, 0.01, 0.1]\n",
    "\n",
    "best_hidden_size = None\n",
    "best_lr = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    \n",
    "    for lr in lr_values:\n",
    "        model = BinaryClassifier(input_size, hidden_size, lr, num_epochs)\n",
    "        model.train()\n",
    "        train_model(model, train_loader, val_loader)\n",
    "        y_pred = predict(model, val_loader)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_hidden_size = hidden_size\n",
    "            best_lr = lr\n",
    "            best_accuracy = accuracy\n",
    "            \n",
    "print(f\"Best Hidden Size: {best_hidden_size}\")\n",
    "print(f\"Best Learning Rate: {best_lr}\")\n",
    "print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.7242, Val Loss: 0.7182\n",
      "Epoch [2/100], Loss: 0.7008, Val Loss: 0.6846\n",
      "Epoch [3/100], Loss: 0.6500, Val Loss: 0.6556\n",
      "Epoch [4/100], Loss: 0.6232, Val Loss: 0.6269\n",
      "Epoch [5/100], Loss: 0.6062, Val Loss: 0.5986\n",
      "Epoch [6/100], Loss: 0.5725, Val Loss: 0.5709\n",
      "Epoch [7/100], Loss: 0.5410, Val Loss: 0.5451\n",
      "Epoch [8/100], Loss: 0.5252, Val Loss: 0.5187\n",
      "Epoch [9/100], Loss: 0.5023, Val Loss: 0.4946\n",
      "Epoch [10/100], Loss: 0.4909, Val Loss: 0.4723\n",
      "Epoch [11/100], Loss: 0.4654, Val Loss: 0.4523\n",
      "Epoch [12/100], Loss: 0.4508, Val Loss: 0.4331\n",
      "Epoch [13/100], Loss: 0.4236, Val Loss: 0.4168\n",
      "Epoch [14/100], Loss: 0.3961, Val Loss: 0.4012\n",
      "Epoch [15/100], Loss: 0.3809, Val Loss: 0.3871\n",
      "Epoch [16/100], Loss: 0.3631, Val Loss: 0.3740\n",
      "Epoch [17/100], Loss: 0.3579, Val Loss: 0.3629\n",
      "Epoch [18/100], Loss: 0.3470, Val Loss: 0.3528\n",
      "Epoch [19/100], Loss: 0.3475, Val Loss: 0.3444\n",
      "Epoch [20/100], Loss: 0.3739, Val Loss: 0.3372\n",
      "Epoch [21/100], Loss: 0.4355, Val Loss: 0.3323\n",
      "Epoch [22/100], Loss: 0.3547, Val Loss: 0.3273\n",
      "Epoch [23/100], Loss: 0.3271, Val Loss: 0.3215\n",
      "Epoch [24/100], Loss: 0.3343, Val Loss: 0.3163\n",
      "Epoch [25/100], Loss: 0.3600, Val Loss: 0.3112\n",
      "Epoch [26/100], Loss: 0.3337, Val Loss: 0.3069\n",
      "Epoch [27/100], Loss: 0.3441, Val Loss: 0.3034\n",
      "Epoch [28/100], Loss: 0.3863, Val Loss: 0.3018\n",
      "Epoch [29/100], Loss: 0.3233, Val Loss: 0.3017\n",
      "Epoch [30/100], Loss: 0.3129, Val Loss: 0.2996\n",
      "Epoch [31/100], Loss: 0.3045, Val Loss: 0.2977\n",
      "Epoch [32/100], Loss: 0.3046, Val Loss: 0.2960\n",
      "Epoch [33/100], Loss: 0.3044, Val Loss: 0.2943\n",
      "Epoch [34/100], Loss: 0.3127, Val Loss: 0.2939\n",
      "Epoch [35/100], Loss: 0.3060, Val Loss: 0.2926\n",
      "Epoch [36/100], Loss: 0.3691, Val Loss: 0.2916\n",
      "Epoch [37/100], Loss: 0.3122, Val Loss: 0.2902\n",
      "Epoch [38/100], Loss: 0.3341, Val Loss: 0.2889\n",
      "Epoch [39/100], Loss: 0.3227, Val Loss: 0.2880\n",
      "Epoch [40/100], Loss: 0.3020, Val Loss: 0.2879\n",
      "Epoch [41/100], Loss: 0.2956, Val Loss: 0.2877\n",
      "Epoch [42/100], Loss: 0.2996, Val Loss: 0.2867\n",
      "Epoch [43/100], Loss: 0.2987, Val Loss: 0.2857\n",
      "Epoch [44/100], Loss: 0.3658, Val Loss: 0.2850\n",
      "Epoch [45/100], Loss: 0.3128, Val Loss: 0.2861\n",
      "Epoch [46/100], Loss: 0.2926, Val Loss: 0.2852\n",
      "Epoch [47/100], Loss: 0.3094, Val Loss: 0.2847\n",
      "Epoch [48/100], Loss: 0.3274, Val Loss: 0.2842\n",
      "Epoch [49/100], Loss: 0.3266, Val Loss: 0.2828\n",
      "Epoch [50/100], Loss: 0.2974, Val Loss: 0.2801\n",
      "Epoch [51/100], Loss: 0.2958, Val Loss: 0.2793\n",
      "Epoch [52/100], Loss: 0.3262, Val Loss: 0.2792\n",
      "Epoch [53/100], Loss: 0.3067, Val Loss: 0.2791\n",
      "Epoch [54/100], Loss: 0.3453, Val Loss: 0.2799\n",
      "Epoch [55/100], Loss: 0.2953, Val Loss: 0.2809\n",
      "Epoch [56/100], Loss: 0.3245, Val Loss: 0.2800\n",
      "Epoch [57/100], Loss: 0.2875, Val Loss: 0.2791\n",
      "Epoch [58/100], Loss: 0.3002, Val Loss: 0.2787\n",
      "Epoch [59/100], Loss: 0.2884, Val Loss: 0.2786\n",
      "Epoch [60/100], Loss: 0.3007, Val Loss: 0.2784\n",
      "Epoch [61/100], Loss: 0.2882, Val Loss: 0.2784\n",
      "Epoch [62/100], Loss: 0.2927, Val Loss: 0.2781\n",
      "Epoch [63/100], Loss: 0.2815, Val Loss: 0.2782\n",
      "Epoch [64/100], Loss: 0.3317, Val Loss: 0.2779\n",
      "Epoch [65/100], Loss: 0.3082, Val Loss: 0.2791\n",
      "Epoch [66/100], Loss: 0.2872, Val Loss: 0.2791\n",
      "Epoch [67/100], Loss: 0.2837, Val Loss: 0.2792\n",
      "Epoch [68/100], Loss: 0.3065, Val Loss: 0.2792\n",
      "Epoch [69/100], Loss: 0.2861, Val Loss: 0.2792\n",
      "Early stopping at epoch 69\n",
      "Accuracy of Neural Network: 0.8804347826086957\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the best hyperparameters\n",
    "model = BinaryClassifier(input_size, best_hidden_size, best_lr, num_epochs)\n",
    "model.train()\n",
    "train_model(model, train_loader, val_loader)\n",
    "\n",
    "# Get predictions and accuracy of the model on the test set\n",
    "y_pred = predict(model, test_loader)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Neural Network: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8922\n"
     ]
    }
   ],
   "source": [
    "# Plot Confusion Matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "def plot_confusion_matrix(conf_matrix, title):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(conf_matrix, \"FNN Confusion Matrix\")\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
