{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from DecisionTree import DecisionTree\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from BaggingClassifier import BaggingClassifier\n",
    "%run DecisionTree.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('heart.csv')\n",
    "# Binary encoding for 'Sex' and 'ExerciseAngina'\n",
    "data['Sex'] = data['Sex'].map({'M': 1, 'F': 0})\n",
    "data['ExerciseAngina'] = data['ExerciseAngina'].map({'Y': 1, 'N': 0})\n",
    "# Extract features and labels\n",
    "X = data.drop(columns=['HeartDisease'])\n",
    "y = data['HeartDisease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding for non-binary categorical features\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "if categorical_cols:  # Only encode if there are categorical features\n",
    "    encoded_array = encoder.fit_transform(X[categorical_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "    X = pd.concat([X.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "#X_val (10%) → Used for hyperparameter tuning.\n",
    "# X_test (20%) → Used for final evaluation.\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=2/3, random_state=42, stratify=y_temp\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune the hyperparameters of the DecisionTree model\n",
    "max_depths = [5, 10, 15, 20, 25]\n",
    "min_samples_splits = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Initialize best hyperparameters\n",
    "best_max_depth = None\n",
    "best_min_samples_split = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# Tune hyperparameters\n",
    "for max_depth in max_depths:\n",
    "    for min_samples_split in min_samples_splits:\n",
    "        model = DecisionTree(max_depth=max_depth, min_sample_split=min_samples_split)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_max_depth = max_depth\n",
    "            best_min_samples_split = min_samples_split\n",
    "            best_accuracy = accuracy\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTree: 0.8152173913043478\n"
     ]
    }
   ],
   "source": [
    "# Train the DecisionTree model with the best hyperparameters\n",
    "model = DecisionTree(max_depth=best_max_depth, min_sample_split=best_min_samples_split)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# get predictions and accuracy of the DecisionTree model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of DecisionTree: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8587\n"
     ]
    }
   ],
   "source": [
    "# train sklearn's DecisionTreeClassifier\n",
    "sklearn_model = DecisionTreeClassifier(random_state=42)\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "\n",
    "# predict on validation set\n",
    "val_set = sklearn_model.predict(X_val)\n",
    "# compute accuracy\n",
    "accuracy = accuracy_score(y_val, val_set)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Bagging Classifier\n",
    "bagging_model = BaggingClassifier(base_learner=DecisionTree, n_estimators=20)\n",
    "bagging_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use X_val to tune the model during training, and use X_test to evaluate the model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Validation Accuracy: 0.9022\n"
     ]
    }
   ],
   "source": [
    "# Predict using Bagging Classifier\n",
    "val_set = bagging_model.predict(X_val)\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_val, val_set)\n",
    "print(f\"Bagging Classifier Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging Classifier evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using Bagging Classifier\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# Compute Evaluation Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Test Accuracy: 0.8913\n",
      "Bagging Classifier F1-Score: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy and F1-score\n",
    "print(f\"Bagging Classifier Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Bagging Classifier F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Confusion Matrix\n",
    "def plot_confusion_matrix(conf_matrix, title):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(conf_matrix, \"Bagging Classifier Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 3, metric: euclidean, Validation Accuracy: 0.8696\n",
      "k: 3, metric: manhattan, Validation Accuracy: 0.9239\n",
      "k: 5, metric: euclidean, Validation Accuracy: 0.8913\n",
      "k: 5, metric: manhattan, Validation Accuracy: 0.9022\n",
      "k: 11, metric: euclidean, Validation Accuracy: 0.8913\n",
      "k: 11, metric: manhattan, Validation Accuracy: 0.8913\n",
      "\n",
      "Best Hyperparameters:\n",
      "Best k: 3\n",
      "Best distance metric: manhattan\n",
      "Best Validation Accuracy: 0.9239\n",
      "Test Accuracy: 0.8533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Training KNN model\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ---------------------------\n",
    "#  Hyperparameter Tuning\n",
    "# --------------------------\n",
    "\n",
    "k_values = [3, 5, 11]\n",
    "distance_metrics = ['euclidean', 'manhattan']\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_k = None\n",
    "best_metric = None\n",
    "best_model = None\n",
    "\n",
    "for k in k_values:\n",
    "    for metric in distance_metrics:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_val_pred = knn.predict(X_val_scaled)\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        print(f\"k: {k}, metric: {metric}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Update best model if current model is better\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_k = k\n",
    "            best_metric = metric\n",
    "            best_model = knn\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"Best k: {best_k}\")\n",
    "print(f\"Best distance metric: {best_metric}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluation on Test Set\n",
    "# ---------------------------\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# train logistic regression\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# predict on validation set\n",
    "val_set = logistic_model.predict(X_val)\n",
    "# compute accuracy\n",
    "accuracy = accuracy_score(y_val, val_set)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design Neural Network\n",
    "\n",
    "# Scale Data and Convert to PyTorch Tensors\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_test_scaled = scaler.transform(X_test)          \n",
    "X_val_scaled = scaler.transform(X_val)            \n",
    "\n",
    "# Convert the scaled NumPy arrays to PyTorch tensors.\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  \n",
    "\n",
    "x_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "x_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Definition\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, learning_rate, num_epochs):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model.learning_rate)\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_threshold = 5\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(model.num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for example, lab in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(example).squeeze()\n",
    "            loss = criterion(output, lab.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        val_loss = validate_model(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{model.num_epochs}], Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        if no_improve_epochs >= early_stopping_threshold:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for example, lab in val_loader:\n",
    "            output = model(example).squeeze()\n",
    "            loss = criterion(output, lab.float())\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for example, _ in test_loader:\n",
    "            output = model(example).squeeze()\n",
    "            predicted = (output >= 0.5).int()  # Threshold at 0.5 to decide between class 0 and 1\n",
    "            predictions.extend(predicted.numpy())\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.7248, Val Loss: 0.6960\n",
      "Epoch [2/100], Loss: 0.7069, Val Loss: 0.6809\n",
      "Epoch [3/100], Loss: 0.6756, Val Loss: 0.6663\n",
      "Epoch [4/100], Loss: 0.6638, Val Loss: 0.6503\n",
      "Epoch [5/100], Loss: 0.6433, Val Loss: 0.6331\n",
      "Epoch [6/100], Loss: 0.6364, Val Loss: 0.6137\n",
      "Epoch [7/100], Loss: 0.6229, Val Loss: 0.5938\n",
      "Epoch [8/100], Loss: 0.6056, Val Loss: 0.5725\n",
      "Epoch [9/100], Loss: 0.5701, Val Loss: 0.5499\n",
      "Epoch [10/100], Loss: 0.5357, Val Loss: 0.5249\n",
      "Epoch [11/100], Loss: 0.5137, Val Loss: 0.4992\n",
      "Epoch [12/100], Loss: 0.4831, Val Loss: 0.4739\n",
      "Epoch [13/100], Loss: 0.4746, Val Loss: 0.4499\n",
      "Epoch [14/100], Loss: 0.4349, Val Loss: 0.4280\n",
      "Epoch [15/100], Loss: 0.4100, Val Loss: 0.4067\n",
      "Epoch [16/100], Loss: 0.4066, Val Loss: 0.3877\n",
      "Epoch [17/100], Loss: 0.3940, Val Loss: 0.3718\n",
      "Epoch [18/100], Loss: 0.4680, Val Loss: 0.3589\n",
      "Epoch [19/100], Loss: 0.3998, Val Loss: 0.3528\n",
      "Epoch [20/100], Loss: 0.3422, Val Loss: 0.3469\n",
      "Epoch [21/100], Loss: 0.3504, Val Loss: 0.3397\n",
      "Epoch [22/100], Loss: 0.3298, Val Loss: 0.3328\n",
      "Epoch [23/100], Loss: 0.3651, Val Loss: 0.3263\n",
      "Epoch [24/100], Loss: 0.3318, Val Loss: 0.3201\n",
      "Epoch [25/100], Loss: 0.3259, Val Loss: 0.3144\n",
      "Epoch [26/100], Loss: 0.3141, Val Loss: 0.3095\n",
      "Epoch [27/100], Loss: 0.3130, Val Loss: 0.3062\n",
      "Epoch [28/100], Loss: 0.3218, Val Loss: 0.3026\n",
      "Epoch [29/100], Loss: 0.3819, Val Loss: 0.3007\n",
      "Epoch [30/100], Loss: 0.3074, Val Loss: 0.2987\n",
      "Epoch [31/100], Loss: 0.3404, Val Loss: 0.2964\n",
      "Epoch [32/100], Loss: 0.4052, Val Loss: 0.2938\n",
      "Epoch [33/100], Loss: 0.3013, Val Loss: 0.2936\n",
      "Epoch [34/100], Loss: 0.3033, Val Loss: 0.2919\n",
      "Epoch [35/100], Loss: 0.3770, Val Loss: 0.2896\n",
      "Epoch [36/100], Loss: 0.3077, Val Loss: 0.2862\n",
      "Epoch [37/100], Loss: 0.3014, Val Loss: 0.2837\n",
      "Epoch [38/100], Loss: 0.3162, Val Loss: 0.2819\n",
      "Epoch [39/100], Loss: 0.4393, Val Loss: 0.2799\n",
      "Epoch [40/100], Loss: 0.2976, Val Loss: 0.2803\n",
      "Epoch [41/100], Loss: 0.3057, Val Loss: 0.2793\n",
      "Epoch [42/100], Loss: 0.3135, Val Loss: 0.2776\n",
      "Epoch [43/100], Loss: 0.2987, Val Loss: 0.2761\n",
      "Epoch [44/100], Loss: 0.2944, Val Loss: 0.2749\n",
      "Epoch [45/100], Loss: 0.2981, Val Loss: 0.2739\n",
      "Epoch [46/100], Loss: 0.3128, Val Loss: 0.2739\n",
      "Epoch [47/100], Loss: 0.3022, Val Loss: 0.2740\n",
      "Epoch [48/100], Loss: 0.3494, Val Loss: 0.2746\n",
      "Epoch [49/100], Loss: 0.2936, Val Loss: 0.2740\n",
      "Epoch [50/100], Loss: 0.2903, Val Loss: 0.2729\n",
      "Epoch [51/100], Loss: 0.2967, Val Loss: 0.2721\n",
      "Epoch [52/100], Loss: 0.2994, Val Loss: 0.2708\n",
      "Epoch [53/100], Loss: 0.3362, Val Loss: 0.2703\n",
      "Epoch [54/100], Loss: 0.2886, Val Loss: 0.2705\n",
      "Epoch [55/100], Loss: 0.2895, Val Loss: 0.2702\n",
      "Epoch [56/100], Loss: 0.2952, Val Loss: 0.2688\n",
      "Epoch [57/100], Loss: 0.2953, Val Loss: 0.2670\n",
      "Epoch [58/100], Loss: 0.2992, Val Loss: 0.2671\n",
      "Epoch [59/100], Loss: 0.2925, Val Loss: 0.2661\n",
      "Epoch [60/100], Loss: 0.2888, Val Loss: 0.2656\n",
      "Epoch [61/100], Loss: 0.4279, Val Loss: 0.2649\n",
      "Epoch [62/100], Loss: 0.2858, Val Loss: 0.2637\n",
      "Epoch [63/100], Loss: 0.3413, Val Loss: 0.2636\n",
      "Epoch [64/100], Loss: 0.2831, Val Loss: 0.2649\n",
      "Epoch [65/100], Loss: 0.2806, Val Loss: 0.2644\n",
      "Epoch [66/100], Loss: 0.3056, Val Loss: 0.2645\n",
      "Epoch [67/100], Loss: 0.2996, Val Loss: 0.2654\n",
      "Epoch [68/100], Loss: 0.2903, Val Loss: 0.2649\n",
      "Early stopping at epoch 68\n",
      "Epoch [1/100], Loss: 0.5537, Val Loss: 0.4411\n",
      "Epoch [2/100], Loss: 0.3709, Val Loss: 0.3274\n",
      "Epoch [3/100], Loss: 0.3869, Val Loss: 0.2936\n",
      "Epoch [4/100], Loss: 0.3069, Val Loss: 0.2888\n",
      "Epoch [5/100], Loss: 0.3467, Val Loss: 0.2929\n",
      "Epoch [6/100], Loss: 0.3025, Val Loss: 0.2810\n",
      "Epoch [7/100], Loss: 0.3322, Val Loss: 0.2793\n",
      "Epoch [8/100], Loss: 0.2946, Val Loss: 0.2806\n",
      "Epoch [9/100], Loss: 0.2862, Val Loss: 0.2778\n",
      "Epoch [10/100], Loss: 0.2867, Val Loss: 0.2758\n",
      "Epoch [11/100], Loss: 0.2716, Val Loss: 0.2760\n",
      "Epoch [12/100], Loss: 0.2964, Val Loss: 0.2785\n",
      "Epoch [13/100], Loss: 0.3431, Val Loss: 0.2904\n",
      "Epoch [14/100], Loss: 0.2680, Val Loss: 0.2932\n",
      "Epoch [15/100], Loss: 0.2768, Val Loss: 0.2849\n",
      "Early stopping at epoch 15\n",
      "Epoch [1/100], Loss: 0.4273, Val Loss: 0.3489\n",
      "Epoch [2/100], Loss: 0.3397, Val Loss: 0.2896\n",
      "Epoch [3/100], Loss: 0.3261, Val Loss: 0.3133\n",
      "Epoch [4/100], Loss: 0.2787, Val Loss: 0.2595\n",
      "Epoch [5/100], Loss: 0.2773, Val Loss: 0.3326\n",
      "Epoch [6/100], Loss: 0.2670, Val Loss: 0.2435\n",
      "Epoch [7/100], Loss: 0.2671, Val Loss: 0.2757\n",
      "Epoch [8/100], Loss: 0.3343, Val Loss: 0.2663\n",
      "Epoch [9/100], Loss: 0.3789, Val Loss: 0.3066\n",
      "Epoch [10/100], Loss: 0.3174, Val Loss: 0.2661\n",
      "Epoch [11/100], Loss: 0.3299, Val Loss: 0.3245\n",
      "Early stopping at epoch 11\n",
      "Epoch [1/100], Loss: 0.7013, Val Loss: 0.6731\n",
      "Epoch [2/100], Loss: 0.6598, Val Loss: 0.6320\n",
      "Epoch [3/100], Loss: 0.6200, Val Loss: 0.5941\n",
      "Epoch [4/100], Loss: 0.5750, Val Loss: 0.5580\n",
      "Epoch [5/100], Loss: 0.5479, Val Loss: 0.5241\n",
      "Epoch [6/100], Loss: 0.5145, Val Loss: 0.4913\n",
      "Epoch [7/100], Loss: 0.4874, Val Loss: 0.4618\n",
      "Epoch [8/100], Loss: 0.4635, Val Loss: 0.4359\n",
      "Epoch [9/100], Loss: 0.4500, Val Loss: 0.4125\n",
      "Epoch [10/100], Loss: 0.4320, Val Loss: 0.3932\n",
      "Epoch [11/100], Loss: 0.3804, Val Loss: 0.3780\n",
      "Epoch [12/100], Loss: 0.4680, Val Loss: 0.3640\n",
      "Epoch [13/100], Loss: 0.3858, Val Loss: 0.3538\n",
      "Epoch [14/100], Loss: 0.3488, Val Loss: 0.3442\n",
      "Epoch [15/100], Loss: 0.3509, Val Loss: 0.3354\n",
      "Epoch [16/100], Loss: 0.3334, Val Loss: 0.3254\n",
      "Epoch [17/100], Loss: 0.3257, Val Loss: 0.3178\n",
      "Epoch [18/100], Loss: 0.3288, Val Loss: 0.3117\n",
      "Epoch [19/100], Loss: 0.3213, Val Loss: 0.3076\n",
      "Epoch [20/100], Loss: 0.3176, Val Loss: 0.3030\n",
      "Epoch [21/100], Loss: 0.3445, Val Loss: 0.2991\n",
      "Epoch [22/100], Loss: 0.3046, Val Loss: 0.2975\n",
      "Epoch [23/100], Loss: 0.3001, Val Loss: 0.2954\n",
      "Epoch [24/100], Loss: 0.3439, Val Loss: 0.2938\n",
      "Epoch [25/100], Loss: 0.2954, Val Loss: 0.2927\n",
      "Epoch [26/100], Loss: 0.3381, Val Loss: 0.2918\n",
      "Epoch [27/100], Loss: 0.2958, Val Loss: 0.2913\n",
      "Epoch [28/100], Loss: 0.2910, Val Loss: 0.2900\n",
      "Epoch [29/100], Loss: 0.2959, Val Loss: 0.2886\n",
      "Epoch [30/100], Loss: 0.2976, Val Loss: 0.2868\n",
      "Epoch [31/100], Loss: 0.2858, Val Loss: 0.2856\n",
      "Epoch [32/100], Loss: 0.2874, Val Loss: 0.2852\n",
      "Epoch [33/100], Loss: 0.3101, Val Loss: 0.2839\n",
      "Epoch [34/100], Loss: 0.2933, Val Loss: 0.2816\n",
      "Epoch [35/100], Loss: 0.3214, Val Loss: 0.2799\n",
      "Epoch [36/100], Loss: 0.4298, Val Loss: 0.2756\n",
      "Epoch [37/100], Loss: 0.3022, Val Loss: 0.2740\n",
      "Epoch [38/100], Loss: 0.2811, Val Loss: 0.2741\n",
      "Epoch [39/100], Loss: 0.2950, Val Loss: 0.2743\n",
      "Epoch [40/100], Loss: 0.2844, Val Loss: 0.2752\n",
      "Epoch [41/100], Loss: 0.3028, Val Loss: 0.2741\n",
      "Epoch [42/100], Loss: 0.2833, Val Loss: 0.2771\n",
      "Early stopping at epoch 42\n",
      "Epoch [1/100], Loss: 0.5400, Val Loss: 0.3662\n",
      "Epoch [2/100], Loss: 0.3400, Val Loss: 0.3016\n",
      "Epoch [3/100], Loss: 0.3536, Val Loss: 0.2833\n",
      "Epoch [4/100], Loss: 0.3402, Val Loss: 0.2826\n",
      "Epoch [5/100], Loss: 0.3256, Val Loss: 0.2562\n",
      "Epoch [6/100], Loss: 0.3180, Val Loss: 0.2648\n",
      "Epoch [7/100], Loss: 0.2984, Val Loss: 0.2634\n",
      "Epoch [8/100], Loss: 0.3275, Val Loss: 0.2787\n",
      "Epoch [9/100], Loss: 0.3604, Val Loss: 0.2980\n",
      "Epoch [10/100], Loss: 0.2752, Val Loss: 0.2615\n",
      "Early stopping at epoch 10\n",
      "Epoch [1/100], Loss: 0.4356, Val Loss: 0.3177\n",
      "Epoch [2/100], Loss: 0.4056, Val Loss: 0.5980\n",
      "Epoch [3/100], Loss: 0.4393, Val Loss: 0.2716\n",
      "Epoch [4/100], Loss: 0.3401, Val Loss: 0.3156\n",
      "Epoch [5/100], Loss: 0.2904, Val Loss: 0.2856\n",
      "Epoch [6/100], Loss: 0.4553, Val Loss: 0.3326\n",
      "Epoch [7/100], Loss: 0.3821, Val Loss: 0.4173\n",
      "Epoch [8/100], Loss: 0.3153, Val Loss: 0.3814\n",
      "Early stopping at epoch 8\n",
      "Epoch [1/100], Loss: 0.6940, Val Loss: 0.6426\n",
      "Epoch [2/100], Loss: 0.6258, Val Loss: 0.5877\n",
      "Epoch [3/100], Loss: 0.5704, Val Loss: 0.5372\n",
      "Epoch [4/100], Loss: 0.5325, Val Loss: 0.4923\n",
      "Epoch [5/100], Loss: 0.4913, Val Loss: 0.4543\n",
      "Epoch [6/100], Loss: 0.4394, Val Loss: 0.4246\n",
      "Epoch [7/100], Loss: 0.4064, Val Loss: 0.3989\n",
      "Epoch [8/100], Loss: 0.3919, Val Loss: 0.3756\n",
      "Epoch [9/100], Loss: 0.3787, Val Loss: 0.3576\n",
      "Epoch [10/100], Loss: 0.3810, Val Loss: 0.3449\n",
      "Epoch [11/100], Loss: 0.3384, Val Loss: 0.3355\n",
      "Epoch [12/100], Loss: 0.3305, Val Loss: 0.3277\n",
      "Epoch [13/100], Loss: 0.3235, Val Loss: 0.3205\n",
      "Epoch [14/100], Loss: 0.3360, Val Loss: 0.3141\n",
      "Epoch [15/100], Loss: 0.3141, Val Loss: 0.3079\n",
      "Epoch [16/100], Loss: 0.3077, Val Loss: 0.3037\n",
      "Epoch [17/100], Loss: 0.3545, Val Loss: 0.3002\n",
      "Epoch [18/100], Loss: 0.3044, Val Loss: 0.2997\n",
      "Epoch [19/100], Loss: 0.2978, Val Loss: 0.2985\n",
      "Epoch [20/100], Loss: 0.3048, Val Loss: 0.2968\n",
      "Epoch [21/100], Loss: 0.2935, Val Loss: 0.2943\n",
      "Epoch [22/100], Loss: 0.2932, Val Loss: 0.2928\n",
      "Epoch [23/100], Loss: 0.3004, Val Loss: 0.2919\n",
      "Epoch [24/100], Loss: 0.3180, Val Loss: 0.2898\n",
      "Epoch [25/100], Loss: 0.3295, Val Loss: 0.2913\n",
      "Epoch [26/100], Loss: 0.3151, Val Loss: 0.2915\n",
      "Epoch [27/100], Loss: 0.2937, Val Loss: 0.2937\n",
      "Epoch [28/100], Loss: 0.3152, Val Loss: 0.2928\n",
      "Epoch [29/100], Loss: 0.2839, Val Loss: 0.2954\n",
      "Early stopping at epoch 29\n",
      "Epoch [1/100], Loss: 0.4613, Val Loss: 0.3224\n",
      "Epoch [2/100], Loss: 0.3191, Val Loss: 0.2971\n",
      "Epoch [3/100], Loss: 0.4015, Val Loss: 0.2980\n",
      "Epoch [4/100], Loss: 0.2895, Val Loss: 0.3224\n",
      "Epoch [5/100], Loss: 0.3002, Val Loss: 0.2887\n",
      "Epoch [6/100], Loss: 0.2738, Val Loss: 0.2704\n",
      "Epoch [7/100], Loss: 0.3708, Val Loss: 0.2798\n",
      "Epoch [8/100], Loss: 0.2742, Val Loss: 0.2992\n",
      "Epoch [9/100], Loss: 0.2546, Val Loss: 0.2893\n",
      "Epoch [10/100], Loss: 0.2482, Val Loss: 0.2712\n",
      "Epoch [11/100], Loss: 0.2486, Val Loss: 0.2682\n",
      "Epoch [12/100], Loss: 0.3324, Val Loss: 0.2720\n",
      "Epoch [13/100], Loss: 0.2707, Val Loss: 0.3176\n",
      "Epoch [14/100], Loss: 0.3358, Val Loss: 0.2847\n",
      "Epoch [15/100], Loss: 0.2577, Val Loss: 0.2705\n",
      "Epoch [16/100], Loss: 0.2446, Val Loss: 0.2755\n",
      "Early stopping at epoch 16\n",
      "Epoch [1/100], Loss: 0.5179, Val Loss: 0.3819\n",
      "Epoch [2/100], Loss: 0.3709, Val Loss: 0.5369\n",
      "Epoch [3/100], Loss: 0.3327, Val Loss: 0.3341\n",
      "Epoch [4/100], Loss: 0.3233, Val Loss: 0.2881\n",
      "Epoch [5/100], Loss: 0.3678, Val Loss: 0.3080\n",
      "Epoch [6/100], Loss: 0.3357, Val Loss: 0.3023\n",
      "Epoch [7/100], Loss: 0.2753, Val Loss: 0.3150\n",
      "Epoch [8/100], Loss: 0.2920, Val Loss: 0.3462\n",
      "Epoch [9/100], Loss: 0.2895, Val Loss: 0.2837\n",
      "Epoch [10/100], Loss: 0.2754, Val Loss: 0.3017\n",
      "Epoch [11/100], Loss: 0.2596, Val Loss: 0.4448\n",
      "Epoch [12/100], Loss: 0.2159, Val Loss: 0.4356\n",
      "Epoch [13/100], Loss: 0.2455, Val Loss: 0.2877\n",
      "Epoch [14/100], Loss: 0.2466, Val Loss: 0.4573\n",
      "Early stopping at epoch 14\n",
      "Epoch [1/100], Loss: 0.6549, Val Loss: 0.5747\n",
      "Epoch [2/100], Loss: 0.5557, Val Loss: 0.4961\n",
      "Epoch [3/100], Loss: 0.4542, Val Loss: 0.4359\n",
      "Epoch [4/100], Loss: 0.4520, Val Loss: 0.3904\n",
      "Epoch [5/100], Loss: 0.3742, Val Loss: 0.3618\n",
      "Epoch [6/100], Loss: 0.3469, Val Loss: 0.3381\n",
      "Epoch [7/100], Loss: 0.3572, Val Loss: 0.3222\n",
      "Epoch [8/100], Loss: 0.3231, Val Loss: 0.3105\n",
      "Epoch [9/100], Loss: 0.3149, Val Loss: 0.3029\n",
      "Epoch [10/100], Loss: 0.3475, Val Loss: 0.2942\n",
      "Epoch [11/100], Loss: 0.3563, Val Loss: 0.2894\n",
      "Epoch [12/100], Loss: 0.3231, Val Loss: 0.2852\n",
      "Epoch [13/100], Loss: 0.3038, Val Loss: 0.2847\n",
      "Epoch [14/100], Loss: 0.3012, Val Loss: 0.2868\n",
      "Epoch [15/100], Loss: 0.3515, Val Loss: 0.2836\n",
      "Epoch [16/100], Loss: 0.2946, Val Loss: 0.2794\n",
      "Epoch [17/100], Loss: 0.3228, Val Loss: 0.2797\n",
      "Epoch [18/100], Loss: 0.3097, Val Loss: 0.2788\n",
      "Epoch [19/100], Loss: 0.2897, Val Loss: 0.2788\n",
      "Epoch [20/100], Loss: 0.3250, Val Loss: 0.2798\n",
      "Epoch [21/100], Loss: 0.2973, Val Loss: 0.2906\n",
      "Epoch [22/100], Loss: 0.3109, Val Loss: 0.2877\n",
      "Epoch [23/100], Loss: 0.2808, Val Loss: 0.2862\n",
      "Early stopping at epoch 23\n",
      "Epoch [1/100], Loss: 0.4722, Val Loss: 0.3064\n",
      "Epoch [2/100], Loss: 0.4960, Val Loss: 0.2992\n",
      "Epoch [3/100], Loss: 0.3002, Val Loss: 0.3235\n",
      "Epoch [4/100], Loss: 0.2994, Val Loss: 0.2985\n",
      "Epoch [5/100], Loss: 0.2828, Val Loss: 0.2895\n",
      "Epoch [6/100], Loss: 0.3215, Val Loss: 0.2625\n",
      "Epoch [7/100], Loss: 0.2650, Val Loss: 0.2303\n",
      "Epoch [8/100], Loss: 0.2937, Val Loss: 0.2501\n",
      "Epoch [9/100], Loss: 0.2666, Val Loss: 0.2559\n",
      "Epoch [10/100], Loss: 0.2423, Val Loss: 0.2705\n",
      "Epoch [11/100], Loss: 0.2571, Val Loss: 0.2538\n",
      "Epoch [12/100], Loss: 0.3434, Val Loss: 0.2846\n",
      "Early stopping at epoch 12\n",
      "Epoch [1/100], Loss: 0.5959, Val Loss: 0.2516\n",
      "Epoch [2/100], Loss: 0.3873, Val Loss: 0.4000\n",
      "Epoch [3/100], Loss: 0.3915, Val Loss: 0.4636\n",
      "Epoch [4/100], Loss: 0.4903, Val Loss: 0.3858\n",
      "Epoch [5/100], Loss: 0.2983, Val Loss: 0.2664\n",
      "Epoch [6/100], Loss: 0.3389, Val Loss: 0.3455\n",
      "Early stopping at epoch 6\n",
      "Best Hidden Size: 16\n",
      "Best Learning Rate: 0.001\n",
      "Best Validation Accuracy: 0.9239\n"
     ]
    }
   ],
   "source": [
    "# Tune Hyperparameters \n",
    "input_size = X_train.shape[1]\n",
    "num_epochs = 100\n",
    "hidden_sizes = [16, 32, 64, 128]\n",
    "lr_values = [0.001, 0.01, 0.1]\n",
    "\n",
    "best_hidden_size = None\n",
    "best_lr = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    \n",
    "    for lr in lr_values:\n",
    "        model = BinaryClassifier(input_size, hidden_size, lr, num_epochs)\n",
    "        model.train()\n",
    "        train_model(model, train_loader, val_loader)\n",
    "        y_pred = predict(model, val_loader)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_hidden_size = hidden_size\n",
    "            best_lr = lr\n",
    "            best_accuracy = accuracy\n",
    "            \n",
    "print(f\"Best Hidden Size: {best_hidden_size}\")\n",
    "print(f\"Best Learning Rate: {best_lr}\")\n",
    "print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.7065, Val Loss: 0.6799\n",
      "Epoch [2/100], Loss: 0.6511, Val Loss: 0.6497\n",
      "Epoch [3/100], Loss: 0.6473, Val Loss: 0.6220\n",
      "Epoch [4/100], Loss: 0.6119, Val Loss: 0.5970\n",
      "Epoch [5/100], Loss: 0.5848, Val Loss: 0.5738\n",
      "Epoch [6/100], Loss: 0.5405, Val Loss: 0.5494\n",
      "Epoch [7/100], Loss: 0.5416, Val Loss: 0.5257\n",
      "Epoch [8/100], Loss: 0.5057, Val Loss: 0.5041\n",
      "Epoch [9/100], Loss: 0.4853, Val Loss: 0.4821\n",
      "Epoch [10/100], Loss: 0.4814, Val Loss: 0.4611\n",
      "Epoch [11/100], Loss: 0.4367, Val Loss: 0.4439\n",
      "Epoch [12/100], Loss: 0.4279, Val Loss: 0.4269\n",
      "Epoch [13/100], Loss: 0.4249, Val Loss: 0.4110\n",
      "Epoch [14/100], Loss: 0.3871, Val Loss: 0.3967\n",
      "Epoch [15/100], Loss: 0.4050, Val Loss: 0.3845\n",
      "Epoch [16/100], Loss: 0.3603, Val Loss: 0.3750\n",
      "Epoch [17/100], Loss: 0.3621, Val Loss: 0.3660\n",
      "Epoch [18/100], Loss: 0.3385, Val Loss: 0.3587\n",
      "Epoch [19/100], Loss: 0.3366, Val Loss: 0.3520\n",
      "Epoch [20/100], Loss: 0.3507, Val Loss: 0.3450\n",
      "Epoch [21/100], Loss: 0.3811, Val Loss: 0.3382\n",
      "Epoch [22/100], Loss: 0.3241, Val Loss: 0.3330\n",
      "Epoch [23/100], Loss: 0.3426, Val Loss: 0.3283\n",
      "Epoch [24/100], Loss: 0.3380, Val Loss: 0.3257\n",
      "Epoch [25/100], Loss: 0.3498, Val Loss: 0.3217\n",
      "Epoch [26/100], Loss: 0.3814, Val Loss: 0.3180\n",
      "Epoch [27/100], Loss: 0.3349, Val Loss: 0.3168\n",
      "Epoch [28/100], Loss: 0.3031, Val Loss: 0.3139\n",
      "Epoch [29/100], Loss: 0.3853, Val Loss: 0.3115\n",
      "Epoch [30/100], Loss: 0.3100, Val Loss: 0.3097\n",
      "Epoch [31/100], Loss: 0.3143, Val Loss: 0.3073\n",
      "Epoch [32/100], Loss: 0.3062, Val Loss: 0.3046\n",
      "Epoch [33/100], Loss: 0.3244, Val Loss: 0.3019\n",
      "Epoch [34/100], Loss: 0.3244, Val Loss: 0.3004\n",
      "Epoch [35/100], Loss: 0.2942, Val Loss: 0.2993\n",
      "Epoch [36/100], Loss: 0.2961, Val Loss: 0.2983\n",
      "Epoch [37/100], Loss: 0.2928, Val Loss: 0.2969\n",
      "Epoch [38/100], Loss: 0.2934, Val Loss: 0.2963\n",
      "Epoch [39/100], Loss: 0.4118, Val Loss: 0.2944\n",
      "Epoch [40/100], Loss: 0.3570, Val Loss: 0.2925\n",
      "Epoch [41/100], Loss: 0.3163, Val Loss: 0.2921\n",
      "Epoch [42/100], Loss: 0.3083, Val Loss: 0.2902\n",
      "Epoch [43/100], Loss: 0.2886, Val Loss: 0.2886\n",
      "Epoch [44/100], Loss: 0.3531, Val Loss: 0.2875\n",
      "Epoch [45/100], Loss: 0.2847, Val Loss: 0.2859\n",
      "Epoch [46/100], Loss: 0.3359, Val Loss: 0.2849\n",
      "Epoch [47/100], Loss: 0.2991, Val Loss: 0.2829\n",
      "Epoch [48/100], Loss: 0.2854, Val Loss: 0.2818\n",
      "Epoch [49/100], Loss: 0.2882, Val Loss: 0.2811\n",
      "Epoch [50/100], Loss: 0.2937, Val Loss: 0.2812\n",
      "Epoch [51/100], Loss: 0.2966, Val Loss: 0.2810\n",
      "Epoch [52/100], Loss: 0.2893, Val Loss: 0.2810\n",
      "Epoch [53/100], Loss: 0.3016, Val Loss: 0.2804\n",
      "Epoch [54/100], Loss: 0.3061, Val Loss: 0.2797\n",
      "Epoch [55/100], Loss: 0.2835, Val Loss: 0.2788\n",
      "Epoch [56/100], Loss: 0.3778, Val Loss: 0.2780\n",
      "Epoch [57/100], Loss: 0.2923, Val Loss: 0.2814\n",
      "Epoch [58/100], Loss: 0.3496, Val Loss: 0.2819\n",
      "Epoch [59/100], Loss: 0.3215, Val Loss: 0.2819\n",
      "Epoch [60/100], Loss: 0.2795, Val Loss: 0.2806\n",
      "Epoch [61/100], Loss: 0.3346, Val Loss: 0.2795\n",
      "Early stopping at epoch 61\n",
      "Accuracy of Neural Network: 0.875\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the best hyperparameters\n",
    "model = BinaryClassifier(input_size, best_hidden_size, best_lr, num_epochs)\n",
    "model.train()\n",
    "train_model(model, train_loader, val_loader)\n",
    "\n",
    "# Get predictions and accuracy of the model on the test set\n",
    "y_pred = predict(model, test_loader)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Neural Network: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[69 13]\n",
      " [10 92]]\n",
      "F1 Score: 0.8889\n"
     ]
    }
   ],
   "source": [
    "# evaluate using Conufusion Matrix and f1 score\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
