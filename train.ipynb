{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from DecisionTree import DecisionTree\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "%run DecisionTree.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('heart.csv')\n",
    "# Binary encoding for 'Sex' and 'ExerciseAngina'\n",
    "data['Sex'] = data['Sex'].map({'M': 1, 'F': 0})\n",
    "data['ExerciseAngina'] = data['ExerciseAngina'].map({'Y': 1, 'N': 0})\n",
    "# Extract features and labels\n",
    "X = data.drop(columns=['HeartDisease'])\n",
    "y = data['HeartDisease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for non-binary categorical features\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "if categorical_cols:  # Only encode if there are categorical features\n",
    "    encoded_array = encoder.fit_transform(X[categorical_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "    X = pd.concat([X.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(642, 18) (92, 18) (184, 18)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=2/3, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Train the DecisionTree model\n",
    "model = DecisionTree()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune the hyperparameters of the DecisionTree model\n",
    "max_depths = [5, 10, 15, 20, 25]\n",
    "min_samples_splits = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Initialize best hyperparameters\n",
    "best_max_depth = None\n",
    "best_min_samples_split = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# Tune hyperparameters\n",
    "for max_depth in max_depths:\n",
    "    for min_samples_split in min_samples_splits:\n",
    "        model = DecisionTree(max_depth=max_depth, min_sample_split=min_samples_split)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_max_depth = max_depth\n",
    "            best_min_samples_split = min_samples_split\n",
    "            best_accuracy = accuracy\n",
    "            \n",
    "\n",
    "# Train the DecisionTree model with the best hyperparameters\n",
    "model = DecisionTree(max_depth=best_max_depth, min_sample_split=best_min_samples_split)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTree: 0.7554347826086957\n"
     ]
    }
   ],
   "source": [
    "# get predictiona and accuracy of the DecisionTree model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of DecisionTree: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8587\n"
     ]
    }
   ],
   "source": [
    "# train sklearn's DecisionTreeClassifier\n",
    "sklearn_model = DecisionTreeClassifier(random_state=42)\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "\n",
    "# predict on validation set\n",
    "val_set = sklearn_model.predict(X_val)\n",
    "# compute accuracy\n",
    "accuracy = accuracy_score(y_val, val_set)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bo2dy\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# train logistic regression\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# predict on validation set\n",
    "val_set = logistic_model.predict(X_val)\n",
    "# compute accuracy\n",
    "accuracy = accuracy_score(y_val, val_set)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 18\n",
    "hidden_size = 100 \n",
    "num_classes = 2\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([642, 18])\n",
      "(642, 18)\n",
      "torch.Size([642])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a scaler and scale the data:\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_test_scaled = scaler.transform(X_test)          \n",
    "X_val_scaled = scaler.transform(X_val)            \n",
    "\n",
    "# Convert the scaled NumPy arrays to PyTorch tensors.\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Use torch.long for classification targets.\n",
    "\n",
    "x_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "x_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Print shapes to verify correct transformation:\n",
    "print(X_train_tensor.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning with hidden_size: 16, learning_rate: 0.001\n",
      "Epoch [1/50], Loss: 0.6975, Val Loss: 0.6866\n",
      "Epoch [2/50], Loss: 0.6701, Val Loss: 0.6551\n",
      "Epoch [3/50], Loss: 0.6265, Val Loss: 0.6252\n",
      "Epoch [4/50], Loss: 0.6100, Val Loss: 0.5969\n",
      "Epoch [5/50], Loss: 0.5933, Val Loss: 0.5705\n",
      "Epoch [6/50], Loss: 0.5553, Val Loss: 0.5437\n",
      "Epoch [7/50], Loss: 0.5354, Val Loss: 0.5173\n",
      "Epoch [8/50], Loss: 0.5321, Val Loss: 0.4920\n",
      "Epoch [9/50], Loss: 0.4820, Val Loss: 0.4709\n",
      "Epoch [10/50], Loss: 0.4573, Val Loss: 0.4493\n",
      "Epoch [11/50], Loss: 0.4556, Val Loss: 0.4284\n",
      "Epoch [12/50], Loss: 0.4165, Val Loss: 0.4105\n",
      "Epoch [13/50], Loss: 0.4010, Val Loss: 0.3927\n",
      "Epoch [14/50], Loss: 0.3917, Val Loss: 0.3761\n",
      "Epoch [15/50], Loss: 0.3668, Val Loss: 0.3613\n",
      "Epoch [16/50], Loss: 0.3639, Val Loss: 0.3488\n",
      "Epoch [17/50], Loss: 0.3702, Val Loss: 0.3379\n",
      "Epoch [18/50], Loss: 0.3513, Val Loss: 0.3284\n",
      "Epoch [19/50], Loss: 0.3718, Val Loss: 0.3214\n",
      "Epoch [20/50], Loss: 0.3390, Val Loss: 0.3167\n",
      "Epoch [21/50], Loss: 0.3455, Val Loss: 0.3120\n",
      "Epoch [22/50], Loss: 0.3314, Val Loss: 0.3066\n",
      "Epoch [23/50], Loss: 0.3574, Val Loss: 0.3023\n",
      "Epoch [24/50], Loss: 0.3157, Val Loss: 0.2996\n",
      "Epoch [25/50], Loss: 0.3618, Val Loss: 0.2973\n",
      "Epoch [26/50], Loss: 0.3144, Val Loss: 0.2957\n",
      "Epoch [27/50], Loss: 0.3425, Val Loss: 0.2934\n",
      "Epoch [28/50], Loss: 0.3333, Val Loss: 0.2904\n",
      "Epoch [29/50], Loss: 0.3333, Val Loss: 0.2889\n",
      "Epoch [30/50], Loss: 0.3053, Val Loss: 0.2870\n",
      "Epoch [31/50], Loss: 0.3242, Val Loss: 0.2851\n",
      "Epoch [32/50], Loss: 0.3387, Val Loss: 0.2826\n",
      "Epoch [33/50], Loss: 0.3386, Val Loss: 0.2822\n",
      "Epoch [34/50], Loss: 0.3087, Val Loss: 0.2801\n",
      "Epoch [35/50], Loss: 0.2975, Val Loss: 0.2783\n",
      "Epoch [36/50], Loss: 0.3004, Val Loss: 0.2772\n",
      "Epoch [37/50], Loss: 0.3036, Val Loss: 0.2763\n",
      "Epoch [38/50], Loss: 0.2977, Val Loss: 0.2752\n",
      "Epoch [39/50], Loss: 0.3715, Val Loss: 0.2750\n",
      "Epoch [40/50], Loss: 0.3359, Val Loss: 0.2760\n",
      "Epoch [41/50], Loss: 0.3669, Val Loss: 0.2787\n",
      "Epoch [42/50], Loss: 0.2972, Val Loss: 0.2804\n",
      "Epoch [43/50], Loss: 0.3441, Val Loss: 0.2802\n",
      "Epoch [44/50], Loss: 0.3456, Val Loss: 0.2781\n",
      "Early stopping at epoch 44\n",
      "Validation Accuracy: 0.9022\n",
      "\n",
      "Tuning with hidden_size: 16, learning_rate: 0.01\n",
      "Epoch [1/50], Loss: 0.5586, Val Loss: 0.4277\n",
      "Epoch [2/50], Loss: 0.3810, Val Loss: 0.3037\n",
      "Epoch [3/50], Loss: 0.3881, Val Loss: 0.2692\n",
      "Epoch [4/50], Loss: 0.3215, Val Loss: 0.2674\n",
      "Epoch [5/50], Loss: 0.3841, Val Loss: 0.2706\n",
      "Epoch [6/50], Loss: 0.2985, Val Loss: 0.2819\n",
      "Epoch [7/50], Loss: 0.2911, Val Loss: 0.2846\n",
      "Epoch [8/50], Loss: 0.3502, Val Loss: 0.2840\n",
      "Epoch [9/50], Loss: 0.2814, Val Loss: 0.2870\n",
      "Early stopping at epoch 9\n",
      "Validation Accuracy: 0.8913\n",
      "\n",
      "Tuning with hidden_size: 16, learning_rate: 0.1\n",
      "Epoch [1/50], Loss: 0.4101, Val Loss: 0.5242\n",
      "Epoch [2/50], Loss: 0.3628, Val Loss: 0.2999\n",
      "Epoch [3/50], Loss: 0.2940, Val Loss: 0.3226\n",
      "Epoch [4/50], Loss: 0.2754, Val Loss: 0.3222\n",
      "Epoch [5/50], Loss: 0.2792, Val Loss: 0.3054\n",
      "Epoch [6/50], Loss: 0.2668, Val Loss: 0.2732\n",
      "Epoch [7/50], Loss: 0.3411, Val Loss: 0.2941\n",
      "Epoch [8/50], Loss: 0.3738, Val Loss: 0.3325\n",
      "Epoch [9/50], Loss: 0.3181, Val Loss: 0.3170\n",
      "Epoch [10/50], Loss: 0.2788, Val Loss: 0.3303\n",
      "Epoch [11/50], Loss: 0.4348, Val Loss: 0.4215\n",
      "Early stopping at epoch 11\n",
      "Validation Accuracy: 0.8478\n",
      "\n",
      "Tuning with hidden_size: 32, learning_rate: 0.001\n",
      "Epoch [1/50], Loss: 0.6584, Val Loss: 0.6354\n",
      "Epoch [2/50], Loss: 0.6141, Val Loss: 0.5943\n",
      "Epoch [3/50], Loss: 0.5679, Val Loss: 0.5583\n",
      "Epoch [4/50], Loss: 0.5512, Val Loss: 0.5236\n",
      "Epoch [5/50], Loss: 0.5032, Val Loss: 0.4927\n",
      "Epoch [6/50], Loss: 0.4739, Val Loss: 0.4633\n",
      "Epoch [7/50], Loss: 0.4895, Val Loss: 0.4370\n",
      "Epoch [8/50], Loss: 0.4184, Val Loss: 0.4168\n",
      "Epoch [9/50], Loss: 0.4018, Val Loss: 0.3971\n",
      "Epoch [10/50], Loss: 0.3702, Val Loss: 0.3781\n",
      "Epoch [11/50], Loss: 0.3894, Val Loss: 0.3636\n",
      "Epoch [12/50], Loss: 0.3505, Val Loss: 0.3516\n",
      "Epoch [13/50], Loss: 0.3360, Val Loss: 0.3422\n",
      "Epoch [14/50], Loss: 0.3679, Val Loss: 0.3347\n",
      "Epoch [15/50], Loss: 0.5081, Val Loss: 0.3284\n",
      "Epoch [16/50], Loss: 0.3291, Val Loss: 0.3259\n",
      "Epoch [17/50], Loss: 0.3479, Val Loss: 0.3217\n",
      "Epoch [18/50], Loss: 0.4294, Val Loss: 0.3189\n",
      "Epoch [19/50], Loss: 0.3676, Val Loss: 0.3225\n",
      "Epoch [20/50], Loss: 0.3636, Val Loss: 0.3229\n",
      "Epoch [21/50], Loss: 0.3486, Val Loss: 0.3245\n",
      "Epoch [22/50], Loss: 0.3842, Val Loss: 0.3219\n",
      "Epoch [23/50], Loss: 0.3185, Val Loss: 0.3203\n",
      "Early stopping at epoch 23\n",
      "Validation Accuracy: 0.9130\n",
      "\n",
      "Tuning with hidden_size: 32, learning_rate: 0.01\n",
      "Epoch [1/50], Loss: 0.5349, Val Loss: 0.3583\n",
      "Epoch [2/50], Loss: 0.3486, Val Loss: 0.2787\n",
      "Epoch [3/50], Loss: 0.3130, Val Loss: 0.2761\n",
      "Epoch [4/50], Loss: 0.3041, Val Loss: 0.2777\n",
      "Epoch [5/50], Loss: 0.2900, Val Loss: 0.2796\n",
      "Epoch [6/50], Loss: 0.3078, Val Loss: 0.2790\n",
      "Epoch [7/50], Loss: 0.2853, Val Loss: 0.2664\n",
      "Epoch [8/50], Loss: 0.3260, Val Loss: 0.2596\n",
      "Epoch [9/50], Loss: 0.2684, Val Loss: 0.2676\n",
      "Epoch [10/50], Loss: 0.2830, Val Loss: 0.2677\n",
      "Epoch [11/50], Loss: 0.2783, Val Loss: 0.2691\n",
      "Epoch [12/50], Loss: 0.2590, Val Loss: 0.2604\n",
      "Epoch [13/50], Loss: 0.3374, Val Loss: 0.2726\n",
      "Early stopping at epoch 13\n",
      "Validation Accuracy: 0.8913\n",
      "\n",
      "Tuning with hidden_size: 32, learning_rate: 0.1\n",
      "Epoch [1/50], Loss: 0.4075, Val Loss: 0.3937\n",
      "Epoch [2/50], Loss: 0.3430, Val Loss: 0.3145\n",
      "Epoch [3/50], Loss: 0.3005, Val Loss: 0.2958\n",
      "Epoch [4/50], Loss: 0.2791, Val Loss: 0.2912\n",
      "Epoch [5/50], Loss: 0.2571, Val Loss: 0.2918\n",
      "Epoch [6/50], Loss: 0.2881, Val Loss: 0.2718\n",
      "Epoch [7/50], Loss: 0.3177, Val Loss: 0.3133\n",
      "Epoch [8/50], Loss: 0.4136, Val Loss: 0.2869\n",
      "Epoch [9/50], Loss: 0.4308, Val Loss: 0.4391\n",
      "Epoch [10/50], Loss: 0.2923, Val Loss: 0.3107\n",
      "Epoch [11/50], Loss: 0.2476, Val Loss: 0.2737\n",
      "Early stopping at epoch 11\n",
      "Validation Accuracy: 0.9130\n",
      "\n",
      "Best Hyperparameters:\n",
      "Best Hidden Size: 32\n",
      "Best Learning Rate: 0.001\n",
      "Best Validation Accuracy: 0.9130\n"
     ]
    }
   ],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, learning_rate, num_epochs):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        \n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.l2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model.learning_rate)\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_threshold = 5\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(model.num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for example, lab in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(example).squeeze()  # Remove extra dimension if present\n",
    "            loss = criterion(output, lab.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        val_loss = validate_model(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{model.num_epochs}], Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        if no_improve_epochs >= early_stopping_threshold:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for example, lab in val_loader:\n",
    "            output = model(example).squeeze()\n",
    "            loss = criterion(output, lab.float())\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for example, _ in test_loader:\n",
    "            output = model(example).squeeze()\n",
    "            predicted = (output >= 0.5).int()  # Threshold at 0.5 to decide between class 0 and 1\n",
    "            predictions.extend(predicted.numpy())\n",
    "    return predictions\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Hyperparameter Tuning\n",
    "# ---------------------------\n",
    "hidden_sizes = [16, 32]           \n",
    "learning_rates = [0.001, 0.01, 0.1] \n",
    "num_epochs = 50                    \n",
    "\n",
    "best_hidden_size = None\n",
    "best_learning_rate = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTuning with hidden_size: {hidden_size}, learning_rate: {lr}\")\n",
    "        model = BinaryClassifier(input_size=X_train_tensor.shape[1],\n",
    "                                 hidden_size=hidden_size,\n",
    "                                 learning_rate=lr,\n",
    "                                 num_epochs=num_epochs)\n",
    "        train_model(model, train_loader, val_loader)\n",
    "        y_pred = predict(model, val_loader)\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_hidden_size = hidden_size\n",
    "            best_learning_rate = lr\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"Best Hidden Size: {best_hidden_size}\")\n",
    "print(f\"Best Learning Rate: {best_learning_rate}\")\n",
    "print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.6920, Val Loss: 0.6551\n",
      "Epoch [2/50], Loss: 0.6389, Val Loss: 0.6086\n",
      "Epoch [3/50], Loss: 0.6122, Val Loss: 0.5685\n",
      "Epoch [4/50], Loss: 0.5629, Val Loss: 0.5351\n",
      "Epoch [5/50], Loss: 0.5291, Val Loss: 0.5017\n",
      "Epoch [6/50], Loss: 0.5181, Val Loss: 0.4706\n",
      "Epoch [7/50], Loss: 0.4613, Val Loss: 0.4441\n",
      "Epoch [8/50], Loss: 0.4291, Val Loss: 0.4185\n",
      "Epoch [9/50], Loss: 0.4008, Val Loss: 0.3948\n",
      "Epoch [10/50], Loss: 0.4348, Val Loss: 0.3746\n",
      "Epoch [11/50], Loss: 0.3710, Val Loss: 0.3601\n",
      "Epoch [12/50], Loss: 0.3545, Val Loss: 0.3470\n",
      "Epoch [13/50], Loss: 0.3707, Val Loss: 0.3362\n",
      "Epoch [14/50], Loss: 0.4674, Val Loss: 0.3280\n",
      "Epoch [15/50], Loss: 0.3391, Val Loss: 0.3288\n",
      "Epoch [16/50], Loss: 0.3285, Val Loss: 0.3254\n",
      "Epoch [17/50], Loss: 0.3678, Val Loss: 0.3196\n",
      "Epoch [18/50], Loss: 0.3643, Val Loss: 0.3143\n",
      "Epoch [19/50], Loss: 0.4568, Val Loss: 0.3099\n",
      "Epoch [20/50], Loss: 0.3162, Val Loss: 0.3058\n",
      "Epoch [21/50], Loss: 0.3484, Val Loss: 0.3018\n",
      "Epoch [22/50], Loss: 0.3204, Val Loss: 0.2982\n",
      "Epoch [23/50], Loss: 0.3100, Val Loss: 0.2952\n",
      "Epoch [24/50], Loss: 0.3633, Val Loss: 0.2934\n",
      "Epoch [25/50], Loss: 0.3146, Val Loss: 0.2949\n",
      "Epoch [26/50], Loss: 0.3352, Val Loss: 0.2928\n",
      "Epoch [27/50], Loss: 0.3311, Val Loss: 0.2929\n",
      "Epoch [28/50], Loss: 0.3035, Val Loss: 0.2901\n",
      "Epoch [29/50], Loss: 0.3458, Val Loss: 0.2877\n",
      "Epoch [30/50], Loss: 0.3221, Val Loss: 0.2888\n",
      "Epoch [31/50], Loss: 0.2934, Val Loss: 0.2864\n",
      "Epoch [32/50], Loss: 0.2947, Val Loss: 0.2851\n",
      "Epoch [33/50], Loss: 0.3368, Val Loss: 0.2843\n",
      "Epoch [34/50], Loss: 0.2887, Val Loss: 0.2833\n",
      "Epoch [35/50], Loss: 0.2849, Val Loss: 0.2824\n",
      "Epoch [36/50], Loss: 0.2896, Val Loss: 0.2813\n",
      "Epoch [37/50], Loss: 0.2996, Val Loss: 0.2797\n",
      "Epoch [38/50], Loss: 0.2827, Val Loss: 0.2777\n",
      "Epoch [39/50], Loss: 0.2854, Val Loss: 0.2768\n",
      "Epoch [40/50], Loss: 0.2891, Val Loss: 0.2761\n",
      "Epoch [41/50], Loss: 0.2990, Val Loss: 0.2748\n",
      "Epoch [42/50], Loss: 0.3342, Val Loss: 0.2748\n",
      "Epoch [43/50], Loss: 0.3067, Val Loss: 0.2761\n",
      "Epoch [44/50], Loss: 0.2809, Val Loss: 0.2768\n",
      "Epoch [45/50], Loss: 0.2780, Val Loss: 0.2764\n",
      "Epoch [46/50], Loss: 0.2800, Val Loss: 0.2767\n",
      "Early stopping at epoch 46\n",
      "Test Accuracy: 0.8804\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 5. Train Final Model\n",
    "# ---------------------------\n",
    "# Train the final model using the best hyperparameters found\n",
    "final_model = BinaryClassifier(input_size=X_train_tensor.shape[1],\n",
    "                               hidden_size=best_hidden_size,\n",
    "                               learning_rate=best_learning_rate,\n",
    "                               num_epochs=num_epochs)\n",
    "train_model(final_model, train_loader, val_loader)\n",
    "\n",
    "y_pred = predict(final_model, test_loader)\n",
    "y_pred = np.array(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
